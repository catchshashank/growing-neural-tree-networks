{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Growing Neural Tree\n",
        "class GrowingNeuralTree:\n",
        "    def __init__(self, max_depth=6):\n",
        "        self.max_depth = max_depth\n",
        "        self.nodes = []\n",
        "\n",
        "    def _sequential_predict_probs(self, X, y_true_int=None):\n",
        "        \"\"\"\n",
        "        Routing inference:\n",
        "        - Start at root\n",
        "        - If a sample is wrong at a node (during evaluation where truth is known), send it to next node\n",
        "        - Latest node prediction is retained for each sample\n",
        "        \"\"\"\n",
        "        active = np.arange(len(X))\n",
        "        final_probs = np.zeros((len(X), 10), dtype=float)\n",
        "\n",
        "        for node in self.nodes:\n",
        "            if len(active) == 0:\n",
        "                break\n",
        "\n",
        "            p = probs(node, X[active])\n",
        "            final_probs[active] = p\n",
        "\n",
        "            if y_true_int is not None:\n",
        "                pred = pred_int_from_probs(p)\n",
        "                wrong = (pred != y_true_int[active])\n",
        "                active = active[wrong]\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return final_probs\n",
        "\n",
        "    def _misclassified_indices(self, X_train, y_train_int):\n",
        "        p = self._sequential_predict_probs(X_train, y_true_int=y_train_int)\n",
        "        yhat = pred_int_from_probs(p)\n",
        "        return np.where(yhat != y_train_int)[0]\n",
        "\n",
        "    def _log_metrics(self, logs, depth,\n",
        "                     X_train, y_train_int, y_train_oh,\n",
        "                     X_val, y_val_int, y_val_oh,\n",
        "                     X_test, y_test_int, y_test_oh):\n",
        "\n",
        "        p_tr = self._sequential_predict_probs(X_train, y_true_int=y_train_int)\n",
        "        p_va = self._sequential_predict_probs(X_val, y_true_int=y_val_int)\n",
        "        p_te = self._sequential_predict_probs(X_test, y_true_int=y_test_int)\n",
        "\n",
        "        yhat_tr = pred_int_from_probs(p_tr)\n",
        "        yhat_va = pred_int_from_probs(p_va)\n",
        "        yhat_te = pred_int_from_probs(p_te)\n",
        "\n",
        "        tr_acc = float(accuracy_score(y_train_int, yhat_tr))\n",
        "        va_acc = float(accuracy_score(y_val_int, yhat_va))\n",
        "        te_acc = float(accuracy_score(y_test_int, yhat_te))\n",
        "\n",
        "        tr_loss = ce_loss_from_probs(p_tr, y_train_oh)\n",
        "        va_loss = ce_loss_from_probs(p_va, y_val_oh)\n",
        "        te_loss = ce_loss_from_probs(p_te, y_test_oh)\n",
        "\n",
        "        mis = int(np.sum(yhat_tr != y_train_int))\n",
        "\n",
        "        logs[\"depth\"].append(depth)\n",
        "        logs[\"train_loss\"].append(tr_loss)\n",
        "        logs[\"val_loss\"].append(va_loss)\n",
        "        logs[\"test_loss\"].append(te_loss)\n",
        "        logs[\"train_acc\"].append(tr_acc)\n",
        "        logs[\"val_acc\"].append(va_acc)\n",
        "        logs[\"test_acc\"].append(te_acc)\n",
        "        logs[\"train_misclassified\"].append(mis)\n",
        "\n",
        "        print(f\"[Depth {depth}] mis={mis} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f} | test_acc={te_acc:.4f}\")\n",
        "\n",
        "    def fit(self,\n",
        "            X_train, y_train_int, y_train_oh,\n",
        "            X_val, y_val_int, y_val_oh,\n",
        "            X_test, y_test_int, y_test_oh,\n",
        "            root_epochs=10, root_bs=256, root_lr=1e-3,\n",
        "            child_epochs=20, child_bs=128, child_lr=1e-4,\n",
        "            stop_misclassified_threshold=0,\n",
        "            stop_val_acc_threshold=None):\n",
        "\n",
        "        logs = {\n",
        "            \"depth\": [],\n",
        "            \"train_loss\": [], \"val_loss\": [], \"test_loss\": [],\n",
        "            \"train_acc\": [], \"val_acc\": [], \"test_acc\": [],\n",
        "            \"train_misclassified\": []\n",
        "        }\n",
        "\n",
        "        # Root node (single parent node)\n",
        "        root = create_softmax_node(lr=root_lr)\n",
        "        root.fit(X_train, y_train_oh,\n",
        "                 validation_data=(X_val, y_val_oh),\n",
        "                 epochs=root_epochs, batch_size=root_bs, verbose=1)\n",
        "        self.nodes = [root]\n",
        "\n",
        "        self._log_metrics(logs, 0,\n",
        "                          X_train, y_train_int, y_train_oh,\n",
        "                          X_val, y_val_int, y_val_oh,\n",
        "                          X_test, y_test_int, y_test_oh)\n",
        "\n",
        "        # Grow children\n",
        "        for depth in range(1, self.max_depth + 1):\n",
        "            mis_idx = self._misclassified_indices(X_train, y_train_int)\n",
        "\n",
        "            if len(mis_idx) <= stop_misclassified_threshold:\n",
        "                print(\"Stopping: misclassified threshold reached.\")\n",
        "                break\n",
        "\n",
        "            if stop_val_acc_threshold is not None and logs[\"val_acc\"][-1] >= stop_val_acc_threshold:\n",
        "                print(\"Stopping: validation accuracy threshold reached.\")\n",
        "                break\n",
        "\n",
        "            X_mis = X_train[mis_idx]\n",
        "            y_mis_oh = y_train_oh[mis_idx]\n",
        "\n",
        "            child = create_softmax_node(lr=child_lr)\n",
        "            child.fit(X_mis, y_mis_oh,\n",
        "                      validation_data=(X_val, y_val_oh),\n",
        "                      epochs=child_epochs, batch_size=child_bs, verbose=1)\n",
        "\n",
        "            self.nodes.append(child)\n",
        "\n",
        "            self._log_metrics(logs, depth,\n",
        "                              X_train, y_train_int, y_train_oh,\n",
        "                              X_val, y_val_int, y_val_oh,\n",
        "                              X_test, y_test_int, y_test_oh)\n",
        "\n",
        "        return logs"
      ],
      "metadata": {
        "id": "n2FB9z1mQbGN"
      },
      "id": "n2FB9z1mQbGN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}